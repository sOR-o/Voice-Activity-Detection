{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1f378-510d-46a5-b93e-a7f79078e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from funasr import AutoModel\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from speechbrain.inference.VAD import VAD\n",
    "import seaborn as sns\n",
    "from pyannote.core import Segment\n",
    "from pyannote.audio import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ef1d6-4f23-4155-8528-f74c9692ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# silero\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "torch.hub.set_dir('../models/.cache')\n",
    "model_silero, utils_silero = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              onnx=False)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils_silero\n",
    "\n",
    " # pyannote\n",
    "pipeline = Pipeline.from_pretrained (\n",
    "        \"pyannote/voice-activity-detection\",\n",
    "         use_auth_token=\"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\", # Once while downloading the model\n",
    "        cache_dir=\"../models/.cache\"\n",
    "        )\n",
    "\n",
    "# speechbrain\n",
    "vad = VAD.from_hparams(\n",
    "        source=\"speechbrain/vad-crdnn-libriparty\",\n",
    "        savedir=\"../models/.cache\"  # Save the model in a cache folder\n",
    ")\n",
    "\n",
    "# funasr\n",
    "model_funasr = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec1217a-ed3b-4899-bf21-e50d20b2aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/sgeadmin/Saurabh_Kushwaha/vad/\")\n",
    "\n",
    "from helper import vad_inference_pyannote, print_timestamps_pyannote, run_vad_on_noisy_audio_pyannote, visualize_metrics_vs_SNR_pyannote\n",
    "from helper import vad_inference_funasr, convert_to_timestamps_funasr, run_vad_on_noisy_audio_funasr, visualize_metrics_vs_SNR_funasr\n",
    "from helper import vad_inference_silero, print_timestamps_silero, run_vad_on_noisy_audio_silero, visualize_metrics_vs_SNR_silero\n",
    "from helper import vad_inference_speechbrain, print_timestamps_speechbrain, run_vad_on_noisy_audio_speechbrain, visualize_metrics_vs_SNR_speechbrain\n",
    "from helper.vad import parse_annotations_file_bh, evaluate_vad, add_noise, save_audio, plot_SNR, extract_metrics, visualize_all_metrics, evaluate_vad_cmatrix, plot_confusion_matrices, get_file_paths, read_path, parse_annotations_file, average_metrics, show_vad_matrix_bh, save_results_to_csv, extract_speech_segments, count_continuous_zeros_after_start_segments, count_continuous_ones_after_end_segments, calculate_fec, calculate_msc, calculate_over, calculate_nds, save_results_to_csv1, show_vad_metrics_matrix1, save_multiple_speech_segments_as_text, extract_filenames_as_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a03a4e7e-b85f-47d7-a789-2f5c7c8a943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(file_path):\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    file_id = file_name.split('.')[0]\n",
    "    return file_id\n",
    "\n",
    "def parse_speech_segments(file_path):\n",
    "    speech_segments = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            label, start_time, end_time = line.strip().split()\n",
    "            if not label in [\"!SIL\"]:  # Only process lines where the label is 'S'\n",
    "\n",
    "                \n",
    "                speech_segments.append({\n",
    "                    'speech': [round(float(start_time), 6), round(float(end_time), 6)]\n",
    "                })\n",
    "    return speech_segments\n",
    "\n",
    "def generate_speech_segments_from_nonspeech_segments(nonspeech_segments, total_duration, margin=0.001):\n",
    "    speech_segments = []\n",
    "    current_time = 0.0\n",
    "\n",
    "    for nonspeech in nonspeech_segments:\n",
    "        nonspeech_start, nonspeech_end = nonspeech['nonspeech']\n",
    "\n",
    "        if nonspeech_start > current_time:\n",
    "            speech_segments.append({'speech': [round(current_time, 6), round(nonspeech_start - margin, 6)]})\n",
    "\n",
    "        current_time = nonspeech_end + margin\n",
    "\n",
    "    if current_time < total_duration:\n",
    "        speech_segments.append({'speech': [round(current_time, 6), round(total_duration, 6)]})\n",
    "\n",
    "    return speech_segments\n",
    "\n",
    "def get_wav_duration(file_path):\n",
    "    with wave.open(file_path, 'rb') as wav_file:\n",
    "        n_frames = wav_file.getnframes()\n",
    "        frame_rate = wav_file.getframerate()\n",
    "        duration = n_frames / float(frame_rate)\n",
    "        \n",
    "        return duration\n",
    "\n",
    "def extract_speech_segments_from_json(file_path, key):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if key not in data:\n",
    "        return []\n",
    "        \n",
    "    sil_value = data[key][\"sil\"]\n",
    "    parsed_sil = eval(sil_value)\n",
    "    output = [{'nonspeech': [float(num) for num in sublist]} for sublist in parsed_sil]\n",
    "\n",
    "    wav_file = \"/Users/saurabh/Documents/projects/Voice-Activity-Detection/data/bh_dataset/bh_audios/\" + key + \".wav\"\n",
    "    total_duraction = get_wav_duration(wav_file)\n",
    "    \n",
    "    output = generate_speech_segments_from_nonspeech_segments(output, total_duraction)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def show_vad_metrics_matrix1(metrics_fec, metrics_msc, metrics_over, metrics_nds, flag):\n",
    "    models = ['Pyannote', 'FunASR', 'Silero', 'SpeechBrain', 'ASRmodel', 'newmodel']\n",
    "    metrics = ['FEC', 'MSC', 'OVER', 'NDS']\n",
    "    \n",
    "    combined_data = {metric: {model: [] for model in models} for metric in metrics}\n",
    "    \n",
    "    for model_name in models:\n",
    "        combined_data['FEC'][model_name] = metrics_fec[model_name]\n",
    "        combined_data['MSC'][model_name] = metrics_msc[model_name]\n",
    "        combined_data['OVER'][model_name] = metrics_over[model_name]\n",
    "        combined_data['NDS'][model_name] = metrics_nds[model_name]\n",
    "    \n",
    "    average_data = {metric: {model: np.mean(combined_data[metric][model]) for model in models} for metric in metrics}\n",
    "    \n",
    "    df_combined = pd.DataFrame(average_data).T\n",
    "    \n",
    "    if flag:\n",
    "        print(df_combined)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"VAD Metrics Comparison\")\n",
    "    sns.heatmap(df_combined, annot=True, cmap=\"YlGnBu\", fmt=\".3f\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "def save_results_to_csv1(metrics_fec, metrics_msc, metrics_over, metrics_nds, model_names, output_file, label_paths):\n",
    "    all_results = []\n",
    "    num_files = len(metrics_fec[model_names[0]])  # Assumes all metrics have the same number of files\n",
    "\n",
    "    for file_idx in range(num_files):\n",
    "        file = label_paths[file_idx].split('.')[0].split('/')[-1]\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            fec_value = metrics_fec[model_name][file_idx]\n",
    "            msc_value = metrics_msc[model_name][file_idx]\n",
    "            over_value = metrics_over[model_name][file_idx]\n",
    "            nds_value = metrics_nds[model_name][file_idx]\n",
    "\n",
    "            result = {\n",
    "                'model': model_name,\n",
    "                'file index': file_idx,\n",
    "                'audio file': file,\n",
    "                'FEC': fec_value,\n",
    "                'MSC': msc_value,\n",
    "                'OVER': over_value,\n",
    "                'NDS': nds_value\n",
    "            }\n",
    "            all_results.append(result)\n",
    "    \n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f859fa6a-46a3-44fd-9fb1-24875336e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_directory = \"/Users/saurabh/Documents/projects/Voice-Activity-Detection/data/bh_dataset/188_samples/188_audio\"\n",
    "label_directory = \"/Users/saurabh/Documents/projects/Voice-Activity-Detection/data/bh_dataset/188_samples/188_label_for_FE\"\n",
    "transcript_directory = \"/Users/saurabh/Documents/projects/Voice-Activity-Detection/testing/188_trans\"\n",
    "\n",
    "_, label_paths = read_path(wav_directory, label_directory)\n",
    "audio_paths, forced_paths = read_path(wav_directory, transcript_directory)\n",
    "\n",
    "audio_paths.sort()\n",
    "label_paths.sort()\n",
    "forced_paths.sort()\n",
    "\n",
    "annotated_segments = [parse_annotations_file_bh(label_path) for label_path in label_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77b9f147-a4f3-40a8-ad14-71de915d4473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_speech_segments(segments):\n",
    "    speech_segments = [segment for segment in segments if 'speech' in segment]\n",
    "    return speech_segments\n",
    "\n",
    "def merge_speech_segments(speech_segments):\n",
    "    if not speech_segments:\n",
    "        return []\n",
    "\n",
    "    start_time = speech_segments[0]['speech'][0]\n",
    "    end_time = speech_segments[-1]['speech'][1]\n",
    "\n",
    "    return [{'speech': [start_time, end_time]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dd426b1-bad2-4394-a754-5ff1baed6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_silero = []\n",
    "res_speechbrain = []\n",
    "\n",
    "rr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "000acca6-1619-44de-b18c-249142298d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(audio_paths)):\n",
    "    silero = vad_inference_silero(audio_paths[i], model_silero, utils_silero, sampling_rate=SAMPLING_RATE)\n",
    "    speechbrain = vad_inference_speechbrain(audio_paths[i], vad)\n",
    "\n",
    "    silero = print_timestamps_silero(silero)\n",
    "    speechbrain = print_timestamps_speechbrain(speechbrain)\n",
    "\n",
    "\n",
    "    silero = merge_speech_segments(silero)\n",
    "    speechbrain = merge_speech_segments(speechbrain)\n",
    "\n",
    "    \n",
    "    res_silero.append(silero)\n",
    "    res_speechbrain.append(speechbrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f0e2041-501a-4568-8164-66bfd0fc66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.sort(reverse=True)\n",
    "\n",
    "for index in rr:\n",
    "    audio_paths.pop(index)\n",
    "    label_paths.pop(index)\n",
    "    annotated_segments.pop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd58e7f1-8e11-4b6e-a2bc-ae2867216f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = extract_filenames_as_string(label_paths)\n",
    "\n",
    "save_multiple_speech_segments_as_text(res_silero, file_names, \"/Users/saurabh/Documents/projects/Voice-Activity-Detection/data/bh_dataset/predicted_for_FE/silero\")\n",
    "save_multiple_speech_segments_as_text(res_speechbrain, file_names, \"/Users/saurabh/Documents/projects/Voice-Activity-Detection/data/bh_dataset/predicted_for_FE/speechbrain\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
