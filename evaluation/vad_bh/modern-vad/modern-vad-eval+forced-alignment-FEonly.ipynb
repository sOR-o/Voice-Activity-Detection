{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6da1f378-510d-46a5-b93e-a7f79078e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import json \n",
    "import wave\n",
    "import numpy as np\n",
    "from funasr import AutoModel\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from speechbrain.inference.VAD import VAD\n",
    "import seaborn as sns\n",
    "from pyannote.core import Segment\n",
    "from pyannote.audio import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ef1d6-4f23-4155-8528-f74c9692ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# silero\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "torch.hub.set_dir('../models/.cache')\n",
    "model_silero, utils_silero = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              onnx=False)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils_silero\n",
    "\n",
    " # pyannote\n",
    "pipeline = Pipeline.from_pretrained (\n",
    "        \"pyannote/voice-activity-detection\",\n",
    "         use_auth_token=\"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\", # Once while downloading the model\n",
    "        cache_dir=\"../models/.cache\"\n",
    "        )\n",
    "\n",
    "# speechbrain\n",
    "vad = VAD.from_hparams(\n",
    "        source=\"speechbrain/vad-crdnn-libriparty\",\n",
    "        savedir=\"../models/.cache\"  # Save the model in a cache folder\n",
    ")\n",
    "\n",
    "# funasr\n",
    "model_funasr = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ec1217a-ed3b-4899-bf21-e50d20b2aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/sgeadmin/Saurabh_Kushwaha/vad/\")\n",
    "\n",
    "from helper import vad_inference_pyannote, print_timestamps_pyannote, run_vad_on_noisy_audio_pyannote, visualize_metrics_vs_SNR_pyannote\n",
    "from helper import vad_inference_funasr, convert_to_timestamps_funasr, run_vad_on_noisy_audio_funasr, visualize_metrics_vs_SNR_funasr\n",
    "from helper import vad_inference_silero, print_timestamps_silero, run_vad_on_noisy_audio_silero, visualize_metrics_vs_SNR_silero\n",
    "from helper import vad_inference_speechbrain, print_timestamps_speechbrain, run_vad_on_noisy_audio_speechbrain, visualize_metrics_vs_SNR_speechbrain\n",
    "from helper.vad import parse_annotations_file_bh, evaluate_vad, add_noise, save_audio, plot_SNR, extract_metrics, visualize_all_metrics, evaluate_vad_cmatrix, plot_confusion_matrices, get_file_paths, read_path, parse_annotations_file, average_metrics, show_vad_matrix_bh, save_results_to_csv, extract_speech_segments, count_continuous_zeros_after_start_segments, count_continuous_ones_after_end_segments, calculate_fec, calculate_msc, calculate_over, calculate_nds, save_results_to_csv1, show_vad_metrics_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c90b87d4-433b-4c05-a209-d4ecda609e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(file_path):\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    file_id = file_name.split('.')[0]\n",
    "    return file_id\n",
    "\n",
    "def parse_speech_segments(file_path):\n",
    "    speech_segments = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            label, start_time, end_time = line.strip().split()\n",
    "            if not label in [\"!SIL\"]:  # Only process lines where the label is 'S'\n",
    "\n",
    "                \n",
    "                speech_segments.append({\n",
    "                    'speech': [round(float(start_time), 6), round(float(end_time), 6)]\n",
    "                })\n",
    "    return speech_segments\n",
    "\n",
    "def generate_speech_segments_from_nonspeech_segments(nonspeech_segments, total_duration, margin=0.001):\n",
    "    speech_segments = []\n",
    "    current_time = 0.0\n",
    "\n",
    "    for nonspeech in nonspeech_segments:\n",
    "        nonspeech_start, nonspeech_end = nonspeech['nonspeech']\n",
    "\n",
    "        if nonspeech_start > current_time:\n",
    "            speech_segments.append({'speech': [round(current_time, 6), round(nonspeech_start - margin, 6)]})\n",
    "\n",
    "        current_time = nonspeech_end + margin\n",
    "\n",
    "    if current_time < total_duration:\n",
    "        speech_segments.append({'speech': [round(current_time, 6), round(total_duration, 6)]})\n",
    "\n",
    "    return speech_segments\n",
    "\n",
    "def get_wav_duration(file_path):\n",
    "    with wave.open(file_path, 'rb') as wav_file:\n",
    "        n_frames = wav_file.getnframes()\n",
    "        frame_rate = wav_file.getframerate()\n",
    "        duration = n_frames / float(frame_rate)\n",
    "        \n",
    "        return duration\n",
    "\n",
    "def extract_speech_segments_from_json(file_path, key):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if key not in data:\n",
    "        return []\n",
    "        \n",
    "    sil_value = data[key][\"sil\"]\n",
    "    parsed_sil = eval(sil_value)\n",
    "    output = [{'nonspeech': [float(num) for num in sublist]} for sublist in parsed_sil]\n",
    "\n",
    "    wav_file = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/188_samples/188_audio/\" + key + \".wav\"\n",
    "    total_duraction = get_wav_duration(wav_file)\n",
    "    \n",
    "    output = generate_speech_segments_from_nonspeech_segments(output, total_duraction)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def show_vad_metrics_matrix1(metrics_fec, metrics_msc, metrics_over, metrics_nds, flag):\n",
    "    models = ['Pyannote', 'FunASR', 'Silero', 'SpeechBrain', 'ASRmodel', 'newmodel']\n",
    "    metrics = ['FB', 'FA', 'BB', 'BA']\n",
    "    \n",
    "    combined_data = {metric: {model: [] for model in models} for metric in metrics}\n",
    "    \n",
    "    for model_name in models:\n",
    "        combined_data['FB'][model_name] = metrics_fec[model_name]\n",
    "        combined_data['FA'][model_name] = metrics_msc[model_name]\n",
    "        combined_data['BB'][model_name] = metrics_over[model_name]\n",
    "        combined_data['BA'][model_name] = metrics_nds[model_name]\n",
    "    \n",
    "    average_data = {metric: {model: np.mean(combined_data[metric][model]) for model in models} for metric in metrics}\n",
    "    \n",
    "    df_combined = pd.DataFrame(average_data).T\n",
    "    \n",
    "    if flag:\n",
    "        print(df_combined)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"VAD Metrics Comparison\")\n",
    "    sns.heatmap(df_combined, annot=True, cmap=\"YlGnBu\", fmt=\".3f\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "def find_start_end(b_arr):\n",
    "    start = b_arr.index(1)\n",
    "    end = len(b_arr) - b_arr[::-1].index(1) - 1\n",
    "    return start, end\n",
    "\n",
    "\n",
    "def front_before(ypred, ytrue):\n",
    "    true_start, _ = find_start_end(ytrue)\n",
    "    pred_start, _ = find_start_end(ypred)\n",
    "    \n",
    "    if true_start - pred_start > 0:\n",
    "        clipping = true_start - pred_start\n",
    "    else:\n",
    "        clipping = 0\n",
    "    return clipping\n",
    "    \n",
    "def front_after(ypred, ytrue):\n",
    "    true_start, _ = find_start_end(ytrue)\n",
    "    pred_start, _ = find_start_end(ypred)\n",
    "    \n",
    "    if pred_start - true_start > 0:\n",
    "        clipping = pred_start - true_start\n",
    "    else:\n",
    "        clipping = 0\n",
    "    return clipping\n",
    "    \n",
    "def back_after(ypred, ytrue):\n",
    "    _, true_end = find_start_end(ytrue)\n",
    "    _, pred_end = find_start_end(ypred)\n",
    "    \n",
    "    if pred_end - true_end >= 0:\n",
    "        clipping = pred_end - true_end\n",
    "    else:\n",
    "        clipping = 0\n",
    "    return clipping\n",
    "    \n",
    "def back_before(ypred, ytrue):\n",
    "    _, true_end = find_start_end(ytrue)\n",
    "    _, pred_end = find_start_end(ypred)\n",
    "    \n",
    "    if true_end - pred_end >= 0:\n",
    "        clipping = true_end - pred_end\n",
    "    else:\n",
    "        clipping = 0\n",
    "    return clipping  \n",
    "\n",
    "def merge_speech_segments(speech_segments):\n",
    "    if not speech_segments:\n",
    "        return []\n",
    "\n",
    "    start_time = speech_segments[0]['speech'][0]\n",
    "    end_time = speech_segments[-1]['speech'][1]\n",
    "\n",
    "    return [{'speech': [start_time, end_time]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f859fa6a-46a3-44fd-9fb1-24875336e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_directory = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/188_samples/188_audio\"\n",
    "label_directory = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/188_samples/188_label\"\n",
    "transcript_directory = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/188_samples/188_trans\"\n",
    "file_path = \"/home/sgeadmin/Saurabh_Kushwaha/vad/evaluation/vad_inhouse/vad_bh/output.json\"\n",
    "\n",
    "_, label_paths = read_path(wav_directory, label_directory)\n",
    "audio_paths, forced_paths = read_path(wav_directory, transcript_directory)\n",
    "\n",
    "audio_paths.sort()\n",
    "label_paths.sort()\n",
    "forced_paths.sort()\n",
    "\n",
    "annotated_segments = [parse_annotations_file_bh(label_path) for label_path in label_paths]\n",
    "\n",
    "label_paths = label_paths[:20]\n",
    "audio_paths = audio_paths[:20]\n",
    "annotated_segments = annotated_segments[:20]\n",
    "forced_paths = forced_paths[:20]\n",
    "\n",
    "len(label_paths), len(audio_paths), len(annotated_segments), len(forced_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dd426b1-bad2-4394-a754-5ff1baed6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmatrix_pyannote = []\n",
    "cmatrix_silero = []\n",
    "cmatrix_speechbrain = []\n",
    "cmatrix_funasr = []\n",
    "cmatrix_ASRmodel = []\n",
    "cmatrix_newmodel = []\n",
    "\n",
    "rr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000acca6-1619-44de-b18c-249142298d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(audio_paths)):\n",
    "    pyannote = vad_inference_pyannote(pipeline, audio_paths[i])\n",
    "    funasr = vad_inference_funasr(audio_paths[i], model_funasr)\n",
    "    silero = vad_inference_silero(audio_paths[i], model_silero, utils_silero, sampling_rate=SAMPLING_RATE)\n",
    "    speechbrain = vad_inference_speechbrain(audio_paths[i], vad)\n",
    "\n",
    "    pyannote = print_timestamps_pyannote(pyannote)\n",
    "    funasr = convert_to_timestamps_funasr(funasr)\n",
    "    silero = print_timestamps_silero(silero)\n",
    "    speechbrain = print_timestamps_speechbrain(speechbrain)\n",
    "    ASRmodel = parse_speech_segments(forced_paths[i])\n",
    "    newmodel = extract_speech_segments_from_json(file_path , get_filename(label_paths[i]))\n",
    "\n",
    "    \n",
    "    funasr = merge_speech_segments(funasr)\n",
    "    pyannote = merge_speech_segments(pyannote)\n",
    "    silero = merge_speech_segments(silero)\n",
    "    speechbrain = merge_speech_segments(speechbrain)\n",
    "    ASRmodel = merge_speech_segments(ASRmodel)\n",
    "    newmodel = merge_speech_segments(newmodel)\n",
    "\n",
    "    if(pyannote == [] or funasr == [] or silero == [] or speechbrain == [] or ASRmodel == [] or newmodel == []):\n",
    "        rr.append(i)\n",
    "        continue\n",
    "\n",
    "    cmatrix_pyannote.append(evaluate_vad_cmatrix(pyannote, annotated_segments[i]))\n",
    "    cmatrix_silero.append(evaluate_vad_cmatrix(silero, annotated_segments[i]))\n",
    "    cmatrix_speechbrain.append(evaluate_vad_cmatrix(speechbrain, annotated_segments[i]))\n",
    "    cmatrix_funasr.append(evaluate_vad_cmatrix(funasr, annotated_segments[i]))\n",
    "    cmatrix_ASRmodel.append(evaluate_vad_cmatrix(ASRmodel, annotated_segments[i]))\n",
    "    cmatrix_newmodel.append(evaluate_vad_cmatrix(newmodel, annotated_segments[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f0e2041-501a-4568-8164-66bfd0fc66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.sort(reverse=True)\n",
    "\n",
    "for index in rr:\n",
    "    audio_paths.pop(index)\n",
    "    label_paths.pop(index)\n",
    "    annotated_segments.pop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb4c9ecd-75dd-45b4-bd4e-ff2edf3a364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize additional metrics containers\n",
    "metrics_fec = {\"Pyannote\": [], \"FunASR\": [], \"Silero\": [], \"SpeechBrain\": [], \"ASRmodel\": [], \"newmodel\": []}\n",
    "metrics_msc = {\"Pyannote\": [], \"FunASR\": [], \"Silero\": [], \"SpeechBrain\": [], \"ASRmodel\": [], \"newmodel\": []}\n",
    "metrics_over = {\"Pyannote\": [], \"FunASR\": [], \"Silero\": [], \"SpeechBrain\": [], \"ASRmodel\": [], \"newmodel\": []}\n",
    "metrics_nds = {\"Pyannote\": [], \"FunASR\": [], \"Silero\": [], \"SpeechBrain\": [], \"ASRmodel\": [], \"newmodel\": []}\n",
    "\n",
    "# Evaluate each model on each audio file\n",
    "for i in range(len(audio_paths)):\n",
    "    # Compute additional metrics for Pyannote\n",
    "    metrics_fec[\"Pyannote\"].append(front_before(cmatrix_pyannote[i][1], cmatrix_pyannote[i][0]))\n",
    "    metrics_msc[\"Pyannote\"].append(front_after(cmatrix_pyannote[i][1], cmatrix_pyannote[i][0]))\n",
    "    metrics_over[\"Pyannote\"].append(back_after(cmatrix_pyannote[i][1], cmatrix_pyannote[i][0]))\n",
    "    metrics_nds[\"Pyannote\"].append(back_before(cmatrix_pyannote[i][1], cmatrix_pyannote[i][0]))\n",
    "\n",
    "    # Compute additional metrics for FunASR\n",
    "    metrics_fec[\"FunASR\"].append(front_before(cmatrix_funasr[i][1], cmatrix_funasr[i][0]))\n",
    "    metrics_msc[\"FunASR\"].append(front_after(cmatrix_funasr[i][1], cmatrix_funasr[i][0]))\n",
    "    metrics_over[\"FunASR\"].append(back_after(cmatrix_funasr[i][1], cmatrix_funasr[i][0]))\n",
    "    metrics_nds[\"FunASR\"].append(back_before(cmatrix_funasr[i][1], cmatrix_funasr[i][0]))\n",
    "\n",
    "    # Compute additional metrics for Silero\n",
    "    metrics_fec[\"Silero\"].append(front_before(cmatrix_silero[i][1], cmatrix_silero[i][0]))\n",
    "    metrics_msc[\"Silero\"].append(front_after(cmatrix_silero[i][1], cmatrix_silero[i][0]))\n",
    "    metrics_over[\"Silero\"].append(back_after(cmatrix_silero[i][1], cmatrix_silero[i][0]))\n",
    "    metrics_nds[\"Silero\"].append(back_before(cmatrix_silero[i][1], cmatrix_silero[i][0]))\n",
    "\n",
    "    # Compute additional metrics for SpeechBrain\n",
    "    metrics_fec[\"SpeechBrain\"].append(front_before(cmatrix_speechbrain[i][1], cmatrix_speechbrain[i][0]))\n",
    "    metrics_msc[\"SpeechBrain\"].append(front_after(cmatrix_speechbrain[i][1], cmatrix_speechbrain[i][0]))\n",
    "    metrics_over[\"SpeechBrain\"].append(back_after(cmatrix_speechbrain[i][1], cmatrix_speechbrain[i][0]))\n",
    "    metrics_nds[\"SpeechBrain\"].append(back_before(cmatrix_speechbrain[i][1], cmatrix_speechbrain[i][0]))\n",
    "\n",
    "    # Compute additional metrics for ASRmodel\n",
    "    metrics_fec[\"ASRmodel\"].append(front_before(cmatrix_ASRmodel[i][1], cmatrix_ASRmodel[i][0]))\n",
    "    metrics_msc[\"ASRmodel\"].append(front_after(cmatrix_ASRmodel[i][1], cmatrix_ASRmodel[i][0]))\n",
    "    metrics_over[\"ASRmodel\"].append(back_after(cmatrix_ASRmodel[i][1], cmatrix_ASRmodel[i][0]))\n",
    "    metrics_nds[\"ASRmodel\"].append(back_before(cmatrix_ASRmodel[i][1], cmatrix_ASRmodel[i][0]))\n",
    "\n",
    "    metrics_fec[\"newmodel\"].append(front_before(cmatrix_newmodel[i][1], cmatrix_newmodel[i][0]))\n",
    "    metrics_msc[\"newmodel\"].append(front_after(cmatrix_newmodel[i][1], cmatrix_newmodel[i][0]))\n",
    "    metrics_over[\"newmodel\"].append(back_after(cmatrix_newmodel[i][1], cmatrix_newmodel[i][0]))\n",
    "    metrics_nds[\"newmodel\"].append(back_before(cmatrix_newmodel[i][1], cmatrix_newmodel[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7972d5fa-cb21-4c2e-a23a-5ee97d7caf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_vad_metrics_matrix1(metrics_fec, metrics_msc, metrics_over, metrics_nds, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "630ed0e3-787a-4595-b35b-c7aa6c8bb6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Silero', 'SpeechBrain', 'ASRmodel']\n",
    "save_results_to_csv1(metrics_fec, metrics_msc, metrics_over, metrics_nds, model_names, 'vad-new-bh.csv', label_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a9a90-eae8-4667-94b3-814480e0367b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
