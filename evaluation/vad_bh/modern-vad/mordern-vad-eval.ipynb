{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1f378-510d-46a5-b93e-a7f79078e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from funasr import AutoModel\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from speechbrain.inference.VAD import VAD\n",
    "import seaborn as sns\n",
    "from pyannote.core import Segment\n",
    "from pyannote.audio import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ef1d6-4f23-4155-8528-f74c9692ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# silero\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "torch.hub.set_dir('../models/.cache')\n",
    "model_silero, utils_silero = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              onnx=False)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils_silero\n",
    "\n",
    " # pyannote\n",
    "pipeline = Pipeline.from_pretrained (\n",
    "        \"pyannote/voice-activity-detection\",\n",
    "         use_auth_token=\"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\", # Once while downloading the model\n",
    "        cache_dir=\"../models/.cache\"\n",
    "        )\n",
    "\n",
    "# speechbrain\n",
    "vad = VAD.from_hparams(\n",
    "        source=\"speechbrain/vad-crdnn-libriparty\",\n",
    "        savedir=\"../models/.cache\"  # Save the model in a cache folder\n",
    ")\n",
    "\n",
    "# funasr\n",
    "model_funasr = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec1217a-ed3b-4899-bf21-e50d20b2aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/sgeadmin/Saurabh_Kushwaha/vad/\")\n",
    "\n",
    "from helper import vad_inference_pyannote, print_timestamps_pyannote, run_vad_on_noisy_audio_pyannote, visualize_metrics_vs_SNR_pyannote\n",
    "from helper import vad_inference_funasr, convert_to_timestamps_funasr, run_vad_on_noisy_audio_funasr, visualize_metrics_vs_SNR_funasr\n",
    "from helper import vad_inference_silero, print_timestamps_silero, run_vad_on_noisy_audio_silero, visualize_metrics_vs_SNR_silero\n",
    "from helper import vad_inference_speechbrain, print_timestamps_speechbrain, run_vad_on_noisy_audio_speechbrain, visualize_metrics_vs_SNR_speechbrain\n",
    "from helper.vad import parse_annotations_file_bh, evaluate_vad, add_noise, save_audio, plot_SNR, extract_metrics, visualize_all_metrics, evaluate_vad_cmatrix, plot_confusion_matrices, get_file_paths, read_path, parse_annotations_file, average_metrics, show_vad_matrix_bh, save_results_to_csv, extract_speech_segments, count_continuous_zeros_after_start_segments, count_continuous_ones_after_end_segments, calculate_fec, calculate_msc, calculate_over, calculate_nds, save_results_to_csv1, show_vad_metrics_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f859fa6a-46a3-44fd-9fb1-24875336e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations_file_bh(file_path):\n",
    "    annotated_segments = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    previous_end_time = 0.0  # To track the end time of the previous segment\n",
    "    current_label = 'notspeech'  # Assume initial state is 'notspeech'\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        parts = line.split()\n",
    "        if len(parts) == 3:\n",
    "            start_time = float(parts[0])\n",
    "            end_time = float(parts[1])\n",
    "            label = parts[2]\n",
    "\n",
    "            if label == 'S':\n",
    "                label = 'speech'\n",
    "            else:\n",
    "                continue  # Skip non-speech intervals\n",
    "\n",
    "            # Add \"notspeech\" segment if there's a gap before the current speech\n",
    "            if previous_end_time < start_time:\n",
    "                annotated_segments.append({'notspeech': [previous_end_time, start_time]})\n",
    "\n",
    "            # Add the current speech segment\n",
    "            annotated_segments.append({'speech': [start_time, end_time]})\n",
    "\n",
    "            # Update the previous_end_time to the end of this segment\n",
    "            previous_end_time = end_time\n",
    "            current_label = label\n",
    "\n",
    "    return annotated_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd426b1-bad2-4394-a754-5ff1baed6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pyannote = []\n",
    "result_silero = []\n",
    "result_speechbrain = []\n",
    "result_funasr = []\n",
    "cmatrix_pyannote = []\n",
    "cmatrix_silero = []\n",
    "cmatrix_speechbrain = []\n",
    "cmatrix_funasr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000acca6-1619-44de-b18c-249142298d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_directory = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/bh_audios\"\n",
    "label_directory = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/bh_labels\"\n",
    "\n",
    "\n",
    "audio_paths, label_paths = read_path(wav_directory, label_directory)\n",
    "\n",
    "audio_paths.sort()\n",
    "label_paths.sort()\n",
    "\n",
    "annotated_segments = [parse_annotations_file_bh(label_path) for label_path in label_paths] \n",
    "\n",
    "label_paths = label_paths[:20]\n",
    "audio_paths = audio_paths[:20]\n",
    "annotated_segments = annotated_segments[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb4c9ecd-75dd-45b4-bd4e-ff2edf3a364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmatrix_pyannote = []\n",
    "cmatrix_silero = []\n",
    "cmatrix_speechbrain = []\n",
    "cmatrix_funasr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2e839-b5d9-4f4a-91a8-742d7217c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(audio_paths)):\n",
    "    pyannote = vad_inference_pyannote(pipeline, audio_paths[i])\n",
    "    funasr = vad_inference_funasr(audio_paths[i], model_funasr)\n",
    "    silero = vad_inference_silero(audio_paths[i], model_silero, utils_silero, sampling_rate=SAMPLING_RATE)\n",
    "    speechbrain = vad_inference_speechbrain(audio_paths[i], vad)\n",
    "\n",
    "    pyannote = print_timestamps_pyannote(pyannote)\n",
    "    funasr = convert_to_timestamps_funasr(funasr)\n",
    "    silero = print_timestamps_silero(silero)\n",
    "    speechbrain = print_timestamps_speechbrain(speechbrain)\n",
    "\n",
    "    cmatrix_pyannote.append(evaluate_vad_cmatrix(pyannote, annotated_segments[i]))\n",
    "    cmatrix_silero.append(evaluate_vad_cmatrix(silero, annotated_segments[i]))\n",
    "    cmatrix_speechbrain.append(evaluate_vad_cmatrix(speechbrain, annotated_segments[i]))\n",
    "    cmatrix_funasr.append(evaluate_vad_cmatrix(funasr, annotated_segments[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a7837a7-a736-4ba9-a5ab-b62cfa1d2362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize additional metrics containers\n",
    "metrics_fec = {\"Pyannote\": [], \"FunASR\": [], \"Silero\": [], \"SpeechBrain\": []}\n",
    "metrics_msc = {\"Pyannote\": [], \"FunASR\": [], \"Silero\": [], \"SpeechBrain\": []}\n",
    "metrics_over = {\"Pyannote\": [], \"FunASR\": [], \"Silero\": [], \"SpeechBrain\": []}\n",
    "metrics_nds = {\"Pyannote\": [], \"FunASR\": [], \"Silero\": [], \"SpeechBrain\": []}\n",
    "\n",
    "# Evaluate each model on each audio file\n",
    "for i in range(len(audio_paths)):\n",
    "    # Compute additional metrics\n",
    "    metrics_fec[\"Pyannote\"].append(calculate_fec(cmatrix_pyannote[i][1], cmatrix_pyannote[i][0]))\n",
    "    metrics_msc[\"Pyannote\"].append(calculate_msc(cmatrix_pyannote[i][1], cmatrix_pyannote[i][0]))\n",
    "    metrics_over[\"Pyannote\"].append(calculate_over(cmatrix_pyannote[i][1], cmatrix_pyannote[i][0]))\n",
    "    metrics_nds[\"Pyannote\"].append(calculate_nds(cmatrix_pyannote[i][1], cmatrix_pyannote[i][0]))\n",
    "\n",
    "    metrics_fec[\"FunASR\"].append(calculate_fec(cmatrix_funasr[i][1], cmatrix_funasr[i][0]))\n",
    "    metrics_msc[\"FunASR\"].append(calculate_msc(cmatrix_funasr[i][1], cmatrix_funasr[i][0]))\n",
    "    metrics_over[\"FunASR\"].append(calculate_over(cmatrix_funasr[i][1], cmatrix_funasr[i][0]))\n",
    "    metrics_nds[\"FunASR\"].append(calculate_nds(cmatrix_funasr[i][1], cmatrix_funasr[i][0]))\n",
    "\n",
    "    metrics_fec[\"Silero\"].append(calculate_fec(cmatrix_silero[i][1], cmatrix_silero[i][0]))\n",
    "    metrics_msc[\"Silero\"].append(calculate_msc(cmatrix_silero[i][1], cmatrix_silero[i][0]))\n",
    "    metrics_over[\"Silero\"].append(calculate_over(cmatrix_silero[i][1], cmatrix_silero[i][0]))\n",
    "    metrics_nds[\"Silero\"].append(calculate_nds(cmatrix_silero[i][1], cmatrix_silero[i][0]))\n",
    "\n",
    "    metrics_fec[\"SpeechBrain\"].append(calculate_fec(cmatrix_speechbrain[i][1], cmatrix_speechbrain[i][0]))\n",
    "    metrics_msc[\"SpeechBrain\"].append(calculate_msc(cmatrix_speechbrain[i][1], cmatrix_speechbrain[i][0]))\n",
    "    metrics_over[\"SpeechBrain\"].append(calculate_over(cmatrix_speechbrain[i][1], cmatrix_speechbrain[i][0]))\n",
    "    metrics_nds[\"SpeechBrain\"].append(calculate_nds(cmatrix_speechbrain[i][1], cmatrix_speechbrain[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa93488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_vad_metrics_matrix1(metrics_fec, metrics_msc, metrics_over, metrics_nds, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b0db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Pyannote', 'FunASR', 'Silero', 'SpeechBrain']\n",
    "save_results_to_csv1(metrics_fec, metrics_msc, metrics_over, metrics_nds, model_names, 'vad-new-bh.csv', label_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
