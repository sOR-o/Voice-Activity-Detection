{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1f378-510d-46a5-b93e-a7f79078e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from funasr import AutoModel\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from speechbrain.inference.VAD import VAD\n",
    "import seaborn as sns\n",
    "from pyannote.core import Segment\n",
    "from pyannote.audio import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304ef1d6-4f23-4155-8528-f74c9692ac76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to ../models/.cache/master.zip\n",
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.1.3 to v2.3.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/059e96f964841d40f1a5e755bb7223f76666bba4/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.7.1, yours is 2.3.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 14:16:14,336 - modelscope - INFO - PyTorch version 2.3.1 Found.\n",
      "2024-10-20 14:16:14,337 - modelscope - INFO - Loading ast index from /Users/saurabh/.cache/modelscope/ast_indexer\n",
      "2024-10-20 14:16:14,449 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 270895fc7d76b5c7655183a5b4e2f1dd and a total number of 980 components indexed\n",
      "2024-10-20 14:16:16,108 - modelscope - INFO - Use user-specified model revision: v2.0.4\n"
     ]
    }
   ],
   "source": [
    "# silero\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "torch.hub.set_dir('../models/.cache')\n",
    "model_silero, utils_silero = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              onnx=False)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils_silero\n",
    "\n",
    " # pyannote\n",
    "pipeline = Pipeline.from_pretrained (\n",
    "        \"pyannote/voice-activity-detection\",\n",
    "         use_auth_token=\"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\", # Once while downloading the model\n",
    "        cache_dir=\"../models/.cache\"\n",
    "        )\n",
    "\n",
    "# speechbrain\n",
    "vad = VAD.from_hparams(\n",
    "        source=\"speechbrain/vad-crdnn-libriparty\",\n",
    "        savedir=\"../models/.cache\"  # Save the model in a cache folder\n",
    ")\n",
    "\n",
    "# funasr\n",
    "model_funasr = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec1217a-ed3b-4899-bf21-e50d20b2aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/sgeadmin/Saurabh_Kushwaha/vad/\")\n",
    "\n",
    "from helper import vad_inference_pyannote, print_timestamps_pyannote, run_vad_on_noisy_audio_pyannote, visualize_metrics_vs_SNR_pyannote\n",
    "from helper import vad_inference_funasr, convert_to_timestamps_funasr, run_vad_on_noisy_audio_funasr, visualize_metrics_vs_SNR_funasr\n",
    "from helper import vad_inference_silero, print_timestamps_silero, run_vad_on_noisy_audio_silero, visualize_metrics_vs_SNR_silero\n",
    "from helper import vad_inference_speechbrain, print_timestamps_speechbrain, run_vad_on_noisy_audio_speechbrain, visualize_metrics_vs_SNR_speechbrain\n",
    "from helper.vad import parse_annotations_file_bh, evaluate_vad, add_noise, save_audio, plot_SNR, extract_metrics, visualize_all_metrics, evaluate_vad_cmatrix, plot_confusion_matrices, get_file_paths, read_path, parse_annotations_file, average_metrics, show_vad_matrix_bh, save_results_to_csv, extract_speech_segments, count_continuous_zeros_after_start_segments, count_continuous_ones_after_end_segments, calculate_fec, calculate_msc, calculate_over, calculate_nds, save_results_to_csv1, show_vad_metrics_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90b87d4-433b-4c05-a209-d4ecda609e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_speech_segments(file_path):\n",
    "    speech_segments = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            label, start_time, end_time = line.strip().split()\n",
    "            if not label in [\"!SIL\"]:  # Only process lines where the label is 'S'\n",
    "\n",
    "                \n",
    "                speech_segments.append({\n",
    "                    'speech': [round(float(start_time), 6), round(float(end_time), 6)]\n",
    "                })\n",
    "    return speech_segments\n",
    "    \n",
    "def show_vad_metrics_matrix1(metrics_fec, metrics_msc, metrics_over, metrics_nds, flag):\n",
    "    models = ['Pyannote', 'FunASR', 'Silero', 'SpeechBrain', 'ASRmodel']\n",
    "    metrics = ['FEC', 'MSC', 'OVER', 'NDS']\n",
    "    \n",
    "    combined_data = {metric: {model: [] for model in models} for metric in metrics}\n",
    "    \n",
    "    for model_name in models:\n",
    "        combined_data['FEC'][model_name] = metrics_fec[model_name]\n",
    "        combined_data['MSC'][model_name] = metrics_msc[model_name]\n",
    "        combined_data['OVER'][model_name] = metrics_over[model_name]\n",
    "        combined_data['NDS'][model_name] = metrics_nds[model_name]\n",
    "    \n",
    "    average_data = {metric: {model: np.mean(combined_data[metric][model]) for model in models} for metric in metrics}\n",
    "    df_combined = pd.DataFrame(average_data).T\n",
    "    \n",
    "    if flag:\n",
    "        print(df_combined)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"VAD Metrics Comparison\")\n",
    "    sns.heatmap(df_combined, annot=True, cmap=\"YlGnBu\", fmt=\".3f\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "def save_results_to_csv1(metrics_fec, metrics_msc, metrics_over, metrics_nds, model_names, output_file, label_paths):\n",
    "    all_results = []\n",
    "    num_files = len(metrics_fec[model_names[0]])  # Assumes all metrics have the same number of files\n",
    "\n",
    "    # Iterate over each file\n",
    "    for file_idx in range(num_files):\n",
    "        # Extract the audio file name from the label path\n",
    "        file = label_paths[file_idx].split('.')[0].split('/')[-1]\n",
    "        \n",
    "        # Iterate over each model and gather its corresponding metric values\n",
    "        for model_name in model_names:\n",
    "            fec_value = metrics_fec[model_name][file_idx]\n",
    "            msc_value = metrics_msc[model_name][file_idx]\n",
    "            over_value = metrics_over[model_name][file_idx]\n",
    "            nds_value = metrics_nds[model_name][file_idx]\n",
    "\n",
    "            # Create a result dictionary for the current model and file\n",
    "            result = {\n",
    "                'model': model_name,\n",
    "                'file index': file_idx,\n",
    "                'audio file': file,\n",
    "                'FEC': fec_value,\n",
    "                'MSC': msc_value,\n",
    "                'OVER': over_value,\n",
    "                'NDS': nds_value\n",
    "            }\n",
    "            all_results.append(result)\n",
    "    \n",
    "    # Convert the results into a DataFrame and save as a CSV file\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f859fa6a-46a3-44fd-9fb1-24875336e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_directory = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/188_samples/188_audio\"\n",
    "label_directory = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/188_samples/188_label\"\n",
    "transcript_directory = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/188_samples/188_trans\"\n",
    "\n",
    "_, label_paths = read_path(wav_directory, label_directory)\n",
    "audio_paths, forced_paths = read_path(wav_directory, transcript_directory)\n",
    "\n",
    "audio_paths.sort()\n",
    "label_paths.sort()\n",
    "forced_paths.sort()\n",
    "\n",
    "annotated_segments = [parse_annotations_file_bh(label_path) for label_path in label_paths]\n",
    "\n",
    "label_paths = label_paths[:20]\n",
    "audio_paths = audio_paths[:20]\n",
    "annotated_segments = annotated_segments[:20]\n",
    "forced_paths = forced_paths[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dd426b1-bad2-4394-a754-5ff1baed6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmatrix_pyannote = []\n",
    "cmatrix_silero = []\n",
    "cmatrix_speechbrain = []\n",
    "cmatrix_funasr = []\n",
    "cmatrix_ASRmodel = []\n",
    "\n",
    "rr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000acca6-1619-44de-b18c-249142298d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(audio_paths)):\n",
    "    pyannote = vad_inference_pyannote(pipeline, audio_paths[i])\n",
    "    funasr = vad_inference_funasr(audio_paths[i], model_funasr)\n",
    "    silero = vad_inference_silero(audio_paths[i], model_silero, utils_silero, sampling_rate=SAMPLING_RATE)\n",
    "    speechbrain = vad_inference_speechbrain(audio_paths[i], vad)\n",
    "\n",
    "    pyannote = print_timestamps_pyannote(pyannote)\n",
    "    funasr = convert_to_timestamps_funasr(funasr)\n",
    "    silero = print_timestamps_silero(silero)\n",
    "    speechbrain = print_timestamps_speechbrain(speechbrain)\n",
    "    ASRmodel = parse_speech_segments(forced_paths[i])\n",
    "\n",
    "    if(pyannote == [] or funasr == [] or silero == [] or speechbrain == [] or ASRmodel == []):\n",
    "        rr.append(i)\n",
    "        continue\n",
    "\n",
    "    cmatrix_pyannote.append(evaluate_vad_cmatrix(pyannote, annotated_segments[i]))\n",
    "    cmatrix_silero.append(evaluate_vad_cmatrix(silero, annotated_segments[i]))\n",
    "    cmatrix_speechbrain.append(evaluate_vad_cmatrix(speechbrain, annotated_segments[i]))\n",
    "    cmatrix_funasr.append(evaluate_vad_cmatrix(funasr, annotated_segments[i]))\n",
    "    cmatrix_ASRmodel.append(evaluate_vad_cmatrix(ASRmodel, annotated_segments[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f0e2041-501a-4568-8164-66bfd0fc66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.sort(reverse=True)\n",
    "\n",
    "for index in rr:\n",
    "    audio_paths.pop(index)\n",
    "    label_paths.pop(index)\n",
    "    annotated_segments.pop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb4c9ecd-75dd-45b4-bd4e-ff2edf3a364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize additional metrics containers\n",
    "metrics_fec = {\"Pyannote\": [], \"FunASR\": [], \"Silero\": [], \"SpeechBrain\": [], \"ASRmodel\": []}\n",
    "metrics_msc = {\"Pyannote\": [], \"FunASR\": [], \"Silero\": [], \"SpeechBrain\": [], \"ASRmodel\": []}\n",
    "metrics_over = {\"Pyannote\": [], \"FunASR\": [], \"Silero\": [], \"SpeechBrain\": [], \"ASRmodel\": []}\n",
    "metrics_nds = {\"Pyannote\": [], \"FunASR\": [], \"Silero\": [], \"SpeechBrain\": [], \"ASRmodel\": []}\n",
    "\n",
    "# Evaluate each model on each audio file\n",
    "for i in range(len(audio_paths)):\n",
    "    # Compute additional metrics for Pyannote\n",
    "    metrics_fec[\"Pyannote\"].append(calculate_fec(cmatrix_pyannote[i][1], cmatrix_pyannote[i][0]))\n",
    "    metrics_msc[\"Pyannote\"].append(calculate_msc(cmatrix_pyannote[i][1], cmatrix_pyannote[i][0]))\n",
    "    metrics_over[\"Pyannote\"].append(calculate_over(cmatrix_pyannote[i][1], cmatrix_pyannote[i][0]))\n",
    "    metrics_nds[\"Pyannote\"].append(calculate_nds(cmatrix_pyannote[i][1], cmatrix_pyannote[i][0]))\n",
    "\n",
    "    # Compute additional metrics for FunASR\n",
    "    metrics_fec[\"FunASR\"].append(calculate_fec(cmatrix_funasr[i][1], cmatrix_funasr[i][0]))\n",
    "    metrics_msc[\"FunASR\"].append(calculate_msc(cmatrix_funasr[i][1], cmatrix_funasr[i][0]))\n",
    "    metrics_over[\"FunASR\"].append(calculate_over(cmatrix_funasr[i][1], cmatrix_funasr[i][0]))\n",
    "    metrics_nds[\"FunASR\"].append(calculate_nds(cmatrix_funasr[i][1], cmatrix_funasr[i][0]))\n",
    "\n",
    "    # Compute additional metrics for Silero\n",
    "    metrics_fec[\"Silero\"].append(calculate_fec(cmatrix_silero[i][1], cmatrix_silero[i][0]))\n",
    "    metrics_msc[\"Silero\"].append(calculate_msc(cmatrix_silero[i][1], cmatrix_silero[i][0]))\n",
    "    metrics_over[\"Silero\"].append(calculate_over(cmatrix_silero[i][1], cmatrix_silero[i][0]))\n",
    "    metrics_nds[\"Silero\"].append(calculate_nds(cmatrix_silero[i][1], cmatrix_silero[i][0]))\n",
    "\n",
    "    # Compute additional metrics for SpeechBrain\n",
    "    metrics_fec[\"SpeechBrain\"].append(calculate_fec(cmatrix_speechbrain[i][1], cmatrix_speechbrain[i][0]))\n",
    "    metrics_msc[\"SpeechBrain\"].append(calculate_msc(cmatrix_speechbrain[i][1], cmatrix_speechbrain[i][0]))\n",
    "    metrics_over[\"SpeechBrain\"].append(calculate_over(cmatrix_speechbrain[i][1], cmatrix_speechbrain[i][0]))\n",
    "    metrics_nds[\"SpeechBrain\"].append(calculate_nds(cmatrix_speechbrain[i][1], cmatrix_speechbrain[i][0]))\n",
    "\n",
    "    # Compute additional metrics for ASRmodel\n",
    "    metrics_fec[\"ASRmodel\"].append(calculate_fec(cmatrix_ASRmodel[i][1], cmatrix_ASRmodel[i][0]))\n",
    "    metrics_msc[\"ASRmodel\"].append(calculate_msc(cmatrix_ASRmodel[i][1], cmatrix_ASRmodel[i][0]))\n",
    "    metrics_over[\"ASRmodel\"].append(calculate_over(cmatrix_ASRmodel[i][1], cmatrix_ASRmodel[i][0]))\n",
    "    metrics_nds[\"ASRmodel\"].append(calculate_nds(cmatrix_ASRmodel[i][1], cmatrix_ASRmodel[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7972d5fa-cb21-4c2e-a23a-5ee97d7caf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_vad_metrics_matrix1(metrics_fec, metrics_msc, metrics_over, metrics_nds, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a7837a7-a736-4ba9-a5ab-b62cfa1d2362",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Pyannote', 'FunASR', 'Silero', 'SpeechBrain', 'ASRmodel']\n",
    "save_results_to_csv1(metrics_fec, metrics_msc, metrics_over, metrics_nds, model_names, 'vad-new-vani-FA.csv', label_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
