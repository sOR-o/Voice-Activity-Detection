{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fceeb0b-b1f8-4a81-b0fb-61da197eda3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice: ffmpeg is not installed. torchaudio is used to load audio\n",
      "If you want to use ffmpeg backend to load audio, please install it by:\n",
      "\tsudo apt install ffmpeg # ubuntu\n",
      "\t# brew install ffmpeg # mac\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from funasr import AutoModel\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from speechbrain.inference.VAD import VAD\n",
    "import seaborn as sns\n",
    "from pyannote.core import Segment\n",
    "from pyannote.audio import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a349dc-c43b-42bf-8e9b-924d6db538c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# silero\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "torch.hub.set_dir('../models/.cache')\n",
    "model_silero, utils_silero = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              onnx=False)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils_silero\n",
    "\n",
    " # pyannote\n",
    "pipeline = Pipeline.from_pretrained (\n",
    "        \"pyannote/voice-activity-detection\",\n",
    "         use_auth_token=\"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\", # Once while downloading the model\n",
    "        cache_dir=\"../models/.cache\"\n",
    "        )\n",
    "\n",
    "# speechbrain\n",
    "vad = VAD.from_hparams(\n",
    "        source=\"speechbrain/vad-crdnn-libriparty\",\n",
    "        savedir=\"../models/.cache\"  # Save the model in a cache folder\n",
    ")\n",
    "\n",
    "# funasr\n",
    "model_funasr = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68db8c2b-4d0f-43de-9200-73c1ce1fc179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/sgeadmin/Saurabh_Kushwaha/vad/\")\n",
    "\n",
    "from helper import vad_inference_pyannote, print_timestamps_pyannote, run_vad_on_noisy_audio_pyannote, visualize_metrics_vs_SNR_pyannote\n",
    "from helper import vad_inference_funasr, convert_to_timestamps_funasr, run_vad_on_noisy_audio_funasr, visualize_metrics_vs_SNR_funasr\n",
    "from helper import vad_inference_silero, print_timestamps_silero, run_vad_on_noisy_audio_silero, visualize_metrics_vs_SNR_silero\n",
    "from helper import vad_inference_speechbrain, print_timestamps_speechbrain, run_vad_on_noisy_audio_speechbrain, visualize_metrics_vs_SNR_speechbrain\n",
    "from helper.vad import parse_annotations_file_bh, evaluate_vad, add_noise, save_audio, plot_SNR, extract_metrics, visualize_all_metrics, evaluate_vad_cmatrix, plot_confusion_matrices, get_file_paths, read_path, parse_annotations_file, average_metrics, show_vad_matrix_bh, save_results_to_csv, extract_speech_segments, count_continuous_zeros_after_start_segments, count_continuous_ones_after_end_segments, calculate_fec, calculate_msc, calculate_over, calculate_nds, save_results_to_csv1, show_vad_metrics_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c5006d-8ce8-4c0b-af03-a660fb2b7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_speech_segments(file_path):\n",
    "    speech_segments = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            label, start_time, end_time = line.strip().split()\n",
    "            \n",
    "            speech_segments.append({\n",
    "                'speech': [round(float(start_time), 6), round(float(end_time), 6)]\n",
    "            })\n",
    "    return speech_segments\n",
    "\n",
    "\n",
    "def show_vad_matrix_bh(avg_pyannote, avg_funasr, avg_silero, avg_speechbrain, ASRmodel, flag):\n",
    "    models = ['Pyannote', 'FunASR', 'Silero', 'SpeechBrain', 'ASRmodel']\n",
    "    metrics = ['precision', 'recall', 'f1_score', 'accuracy', 'specificity', 'fdr', 'miss_rate']\n",
    "    \n",
    "    # Initialize a dictionary to store the metric data for each model\n",
    "    combined_data = {metric: {model: [] for model in models} for metric in metrics}\n",
    "    \n",
    "    # Iterate over each model and cmatrix and then over each metric\n",
    "    for model_name, cmatrix in zip(models, [avg_pyannote, avg_funasr, avg_silero, avg_speechbrain, ASRmodel]):\n",
    "        for result in cmatrix:\n",
    "            for metric in metrics:  # Ensure we're iterating over metrics\n",
    "                combined_data[metric][model_name].append(result[metric])\n",
    "    \n",
    "    # Compute the average values for each metric and model\n",
    "    average_data = {metric: {model: np.mean(combined_data[metric][model]) for model in models} for metric in metrics}\n",
    "    \n",
    "    # Convert to a DataFrame and transpose for better visualization\n",
    "    df_combined = pd.DataFrame(average_data).T\n",
    "    \n",
    "    if flag:\n",
    "        print(df_combined)\n",
    "    \n",
    "    # Plotting the heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Model Metrics Comparison\")\n",
    "    sns.heatmap(df_combined, annot=True, cmap=\"YlGnBu\", fmt=\".3f\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "def save_results_to_csv(results, model_names, output_file, label_paths):\n",
    "    all_results = []\n",
    "    \n",
    "    num_files = len(results[0]) \n",
    "\n",
    "    for file_idx in range(num_files):\n",
    "        file = label_paths[file_idx].split('.')[0].split('/')[-1]\n",
    "        \n",
    "        for model_idx, model_name in enumerate(model_names):\n",
    "            result = results[model_idx][file_idx]\n",
    "            temp = {'model': model_name, 'file index': file_idx, 'audio file': file}\n",
    "            \n",
    "            temp.update(result)\n",
    "            all_results.append(temp)\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b84ac14e-d091-4a50-8362-a255457e69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_directory = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/188_samples/188_audio\"\n",
    "label_directory = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/188_samples/188_label\"\n",
    "transcript_directory = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/188_samples/188_trans\"\n",
    "\n",
    "_, label_paths = read_path(wav_directory, label_directory)\n",
    "audio_paths, forced_paths = read_path(wav_directory, transcript_directory)\n",
    "\n",
    "audio_paths.sort()\n",
    "label_paths.sort()\n",
    "forced_paths.sort()\n",
    "\n",
    "annotated_segments = [parse_annotations_file_bh(label_path) for label_path in label_paths]\n",
    "\n",
    "label_paths = label_paths[:20]\n",
    "audio_paths = audio_paths[:20]\n",
    "annotated_segments = annotated_segments[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d64b3a38-fc3a-401f-b6cf-973d22cb9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pyannote = []\n",
    "result_silero = []\n",
    "result_speechbrain = []\n",
    "result_funasr = []\n",
    "result_ASRmodel = []\n",
    "\n",
    "rr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e83b9-0013-409f-95e6-04ee0f646ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(audio_paths)):\n",
    "    pyannote = vad_inference_pyannote(pipeline, audio_paths[i])\n",
    "    funasr = vad_inference_funasr(audio_paths[i], model_funasr)\n",
    "    silero = vad_inference_silero(audio_paths[i], model_silero, utils_silero, sampling_rate=SAMPLING_RATE)\n",
    "    speechbrain = vad_inference_speechbrain(audio_paths[i], vad)\n",
    "    \n",
    "    pyannote = print_timestamps_pyannote(pyannote)\n",
    "    funasr = convert_to_timestamps_funasr(funasr)\n",
    "    silero = print_timestamps_silero(silero)\n",
    "    speechbrain = print_timestamps_speechbrain(speechbrain)\n",
    "    \n",
    "    ASRmodel = parse_speech_segments(forced_paths[i])\n",
    "\n",
    "    if(pyannote == [] or funasr == [] or silero == [] or speechbrain == [] or ASRmodel == []):\n",
    "        rr.append(i)\n",
    "        continue\n",
    "    \n",
    "\n",
    "    result_pyannote.append(evaluate_vad(pyannote, annotated_segments[i]))\n",
    "    result_silero.append(evaluate_vad(silero, annotated_segments[i]))\n",
    "    result_speechbrain.append(evaluate_vad(speechbrain, annotated_segments[i]))\n",
    "    result_funasr.append(evaluate_vad(funasr, annotated_segments[i]))\n",
    "    result_ASRmodel.append(evaluate_vad(ASRmodel, annotated_segments[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddd87054-7af1-45fc-9eb3-9b5a0cdd9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.sort(reverse=True)\n",
    "\n",
    "for index in rr:\n",
    "    audio_paths.pop(index)\n",
    "    label_paths.pop(index)\n",
    "    annotated_segments.pop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c52254a6-c627-4825-b44b-8a1193b556b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_funasr = average_metrics(result_funasr)\n",
    "avg_pyannote = average_metrics(result_pyannote)\n",
    "avg_speechbrain = average_metrics(result_speechbrain)\n",
    "avg_silero = average_metrics(result_silero)\n",
    "avg_ASRmodel = average_metrics(result_ASRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f24cf-cd84-49a0-a792-e191442a8082",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_vad_matrix_bh(avg_pyannote, avg_funasr, avg_silero, avg_speechbrain, avg_ASRmodel, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9f1e92a-2a73-42ec-94ca-38a79dc69c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"vad-matrix-FA.csv\"\n",
    "model_names = ['Pyannote', 'FunASR', 'Silero', 'SpeechBrain', 'ASRmodel']\n",
    "\n",
    "# Save CSV file\n",
    "save_results_to_csv(\n",
    "    [result_pyannote, result_funasr, result_silero, result_speechbrain, result_ASRmodel],\n",
    "    model_names,\n",
    "    output_file,\n",
    "    label_paths\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
