{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fceeb0b-b1f8-4a81-b0fb-61da197eda3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import wave\n",
    "from funasr import AutoModel\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from speechbrain.inference.VAD import VAD\n",
    "import seaborn as sns\n",
    "from pyannote.core import Segment\n",
    "from pyannote.audio import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a349dc-c43b-42bf-8e9b-924d6db538c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# silero\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "torch.hub.set_dir('../models/.cache')\n",
    "model_silero, utils_silero = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              onnx=False)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils_silero\n",
    "\n",
    " # pyannote\n",
    "pipeline = Pipeline.from_pretrained (\n",
    "        \"pyannote/voice-activity-detection\",\n",
    "         use_auth_token=\"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\", # Once while downloading the model\n",
    "        cache_dir=\"../models/.cache\"\n",
    "        )\n",
    "\n",
    "# speechbrain\n",
    "vad = VAD.from_hparams(\n",
    "        source=\"speechbrain/vad-crdnn-libriparty\",\n",
    "        savedir=\"../models/.cache\"  # Save the model in a cache folder\n",
    ")\n",
    "\n",
    "# funasr\n",
    "model_funasr = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68db8c2b-4d0f-43de-9200-73c1ce1fc179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/sgeadmin/Saurabh_Kushwaha/vad/\")\n",
    "\n",
    "from helper import vad_inference_pyannote, print_timestamps_pyannote, run_vad_on_noisy_audio_pyannote, visualize_metrics_vs_SNR_pyannote\n",
    "from helper import vad_inference_funasr, convert_to_timestamps_funasr, run_vad_on_noisy_audio_funasr, visualize_metrics_vs_SNR_funasr\n",
    "from helper import vad_inference_silero, print_timestamps_silero, run_vad_on_noisy_audio_silero, visualize_metrics_vs_SNR_silero\n",
    "from helper import vad_inference_speechbrain, print_timestamps_speechbrain, run_vad_on_noisy_audio_speechbrain, visualize_metrics_vs_SNR_speechbrain\n",
    "from helper.vad import parse_annotations_file_bh, evaluate_vad, add_noise, save_audio, plot_SNR, extract_metrics, visualize_all_metrics, evaluate_vad_cmatrix, plot_confusion_matrices, get_file_paths, read_path, parse_annotations_file, average_metrics, show_vad_matrix_bh, save_results_to_csv, extract_speech_segments, count_continuous_zeros_after_start_segments, count_continuous_ones_after_end_segments, calculate_fec, calculate_msc, calculate_over, calculate_nds, save_results_to_csv1, show_vad_metrics_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c5006d-8ce8-4c0b-af03-a660fb2b7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(file_path):\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    file_id = file_name.split('.')[0]\n",
    "    return file_id\n",
    "\n",
    "\n",
    "def generate_speech_segments_from_nonspeech_segments(nonspeech_segments, total_duration, margin=0.001):\n",
    "    speech_segments = []\n",
    "    current_time = 0.0\n",
    "\n",
    "    for nonspeech in nonspeech_segments:\n",
    "        nonspeech_start, nonspeech_end = nonspeech['nonspeech']\n",
    "\n",
    "        if nonspeech_start > current_time:\n",
    "            speech_segments.append({'speech': [round(current_time, 6), round(nonspeech_start - margin, 6)]})\n",
    "\n",
    "        current_time = nonspeech_end + margin\n",
    "\n",
    "    if current_time < total_duration:\n",
    "        speech_segments.append({'speech': [round(current_time, 6), round(total_duration, 6)]})\n",
    "\n",
    "    return speech_segments\n",
    "\n",
    "def get_wav_duration(file_path):\n",
    "    with wave.open(file_path, 'rb') as wav_file:\n",
    "        n_frames = wav_file.getnframes()\n",
    "        frame_rate = wav_file.getframerate()\n",
    "        duration = n_frames / float(frame_rate)\n",
    "        \n",
    "        return duration\n",
    "\n",
    "def extract_speech_segments_from_json(file_path, key):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if key not in data:\n",
    "        return []\n",
    "        \n",
    "    sil_value = data[key][\"sil\"]\n",
    "    parsed_sil = eval(sil_value)\n",
    "    output = [{'nonspeech': [float(num) for num in sublist]} for sublist in parsed_sil]\n",
    "\n",
    "    wav_file = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/188_samples/188_audio/\" + key + \".wav\"\n",
    "    total_duraction = get_wav_duration(wav_file)\n",
    "    output = generate_speech_segments_from_nonspeech_segments(output, total_duraction)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def parse_speech_segments(file_path):\n",
    "    speech_segments = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            label, start_time, end_time = line.strip().split()\n",
    "            \n",
    "            speech_segments.append({\n",
    "                'speech': [round(float(start_time), 6), round(float(end_time), 6)]\n",
    "            })\n",
    "    return speech_segments\n",
    "\n",
    "\n",
    "def show_vad_matrix_bh(avg_pyannote, avg_funasr, avg_silero, avg_speechbrain, ASRmodel, newmodel, flag):\n",
    "    models = ['Pyannote', 'FunASR', 'Silero', 'SpeechBrain', 'ASRmodel', 'newmodel']\n",
    "    metrics = ['precision', 'recall', 'f1_score', 'accuracy', 'specificity', 'fdr', 'miss_rate']\n",
    "    \n",
    "    combined_data = {metric: {model: [] for model in models} for metric in metrics}\n",
    "    \n",
    "    for model_name, cmatrix in zip(models, [avg_pyannote, avg_funasr, avg_silero, avg_speechbrain, ASRmodel, newmodel]):\n",
    "        for result in cmatrix:\n",
    "            for metric in metrics:  # Ensure we're iterating over metrics\n",
    "                combined_data[metric][model_name].append(result[metric])\n",
    "    \n",
    "    average_data = {metric: {model: np.mean(combined_data[metric][model]) for model in models} for metric in metrics}\n",
    "    \n",
    "    df_combined = pd.DataFrame(average_data).T\n",
    "    \n",
    "    if flag:\n",
    "        print(df_combined)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Model Metrics Comparison\")\n",
    "    sns.heatmap(df_combined, annot=True, cmap=\"YlGnBu\", fmt=\".3f\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "def save_results_to_csv(results, model_names, output_file, label_paths):\n",
    "    all_results = []\n",
    "    \n",
    "    num_files = len(results[0]) \n",
    "\n",
    "    for file_idx in range(num_files):\n",
    "        file = label_paths[file_idx].split('.')[0].split('/')[-1]\n",
    "        \n",
    "        for model_idx, model_name in enumerate(model_names):\n",
    "            result = results[model_idx][file_idx]\n",
    "            temp = {'model': model_name, 'file index': file_idx, 'audio file': file}\n",
    "            \n",
    "            temp.update(result)\n",
    "            all_results.append(temp)\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "def merge_speech_segments(speech_segments):\n",
    "    if not speech_segments:\n",
    "        return []\n",
    "\n",
    "    start_time = speech_segments[0]['speech'][0]\n",
    "    end_time = speech_segments[-1]['speech'][1]\n",
    "\n",
    "    return [{'speech': [start_time, end_time]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b84ac14e-d091-4a50-8362-a255457e69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_directory = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/188_samples/188_audio\"\n",
    "transcript_directory = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/188_samples/188_trans\"\n",
    "label_directory = \"/home/sgeadmin/Saurabh_Kushwaha/vad/data/bh_dataset/188_samples/188_label_for_FE\"\n",
    "file_path = \"/home/sgeadmin/Saurabh_Kushwaha/vad/evaluation/vad_inhouse/vad_bh/output.json\" \n",
    "\n",
    "_, label_paths = read_path(wav_directory, label_directory)\n",
    "audio_paths, forced_paths = read_path(wav_directory, transcript_directory)\n",
    "\n",
    "audio_paths.sort()\n",
    "label_paths.sort()\n",
    "forced_paths.sort\n",
    "\n",
    "annotated_segments = [parse_annotations_file_bh(label_path) for label_path in label_paths]\n",
    "\n",
    "audio_paths = audio_paths[:20]\n",
    "label_paths = label_paths[:20]\n",
    "forced_paths = forced_paths[:20]\n",
    "annotated_segments = annotated_segments[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d64b3a38-fc3a-401f-b6cf-973d22cb9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pyannote = []\n",
    "result_silero = []\n",
    "result_speechbrain = []\n",
    "result_funasr = []\n",
    "result_ASRmodel = []\n",
    "result_newmodel = []\n",
    "\n",
    "rr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e83b9-0013-409f-95e6-04ee0f646ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(audio_paths)):\n",
    "    pyannote = vad_inference_pyannote(pipeline, audio_paths[i])\n",
    "    funasr = vad_inference_funasr(audio_paths[i], model_funasr)\n",
    "    silero = vad_inference_silero(audio_paths[i], model_silero, utils_silero, sampling_rate=SAMPLING_RATE)\n",
    "    speechbrain = vad_inference_speechbrain(audio_paths[i], vad)\n",
    "    \n",
    "    pyannote = print_timestamps_pyannote(pyannote)\n",
    "    funasr = convert_to_timestamps_funasr(funasr)\n",
    "    silero = print_timestamps_silero(silero)\n",
    "    speechbrain = print_timestamps_speechbrain(speechbrain)\n",
    "    ASRmodel = parse_speech_segments(forced_paths[i])\n",
    "    newmodel = extract_speech_segments_from_json(file_path , get_filename(audio_paths[i]))\n",
    "\n",
    "    funasr = merge_speech_segments(funasr)\n",
    "    pyannote = merge_speech_segments(pyannote)\n",
    "    silero = merge_speech_segments(silero)\n",
    "    speechbrain = merge_speech_segments(speechbrain)\n",
    "    ASRmodel = merge_speech_segments(ASRmodel)\n",
    "    newmodel = merge_speech_segments(newmodel) \n",
    "\n",
    "    if(pyannote != [] and funasr != [] and silero != [] and speechbrain != [] and ASRmodel != [] and newmodel != []):\n",
    "        result_pyannote.append(evaluate_vad(pyannote, annotated_segments[i]))\n",
    "        result_silero.append(evaluate_vad(silero, annotated_segments[i]))\n",
    "        result_speechbrain.append(evaluate_vad(speechbrain, annotated_segments[i]))\n",
    "        result_funasr.append(evaluate_vad(funasr, annotated_segments[i]))\n",
    "        \n",
    "        result_ASRmodel.append(evaluate_vad(ASRmodel, annotated_segments[i]))\n",
    "        result_newmodel.append(evaluate_vad(newmodel, annotated_segments[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddd87054-7af1-45fc-9eb3-9b5a0cdd9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.sort(reverse=True)\n",
    "\n",
    "for index in rr:\n",
    "    audio_paths.pop(index)\n",
    "    label_paths.pop(index)\n",
    "    annotated_segments.pop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c52254a6-c627-4825-b44b-8a1193b556b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_funasr = average_metrics(result_funasr)\n",
    "avg_pyannote = average_metrics(result_pyannote)\n",
    "avg_speechbrain = average_metrics(result_speechbrain)\n",
    "avg_silero = average_metrics(result_silero)\n",
    "avg_ASRmodel = average_metrics(result_ASRmodel)\n",
    "avg_newmodel = average_metrics(result_newmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f24cf-cd84-49a0-a792-e191442a8082",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_vad_matrix_bh(avg_pyannote, avg_funasr, avg_silero, avg_speechbrain, avg_ASRmodel, avg_newmodel, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36655db3-ef56-48fe-9fa0-1cd3d965cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"vad-matrix-FA.csv\"\n",
    "model_names = ['Pyannote', 'FunASR', 'Silero', 'SpeechBrain', 'ASRmodel']\n",
    "\n",
    "# Save CSV file\n",
    "save_results_to_csv(\n",
    "    [result_pyannote, result_funasr, result_silero, result_speechbrain, result_ASRmodel],\n",
    "    model_names,\n",
    "    output_file,\n",
    "    label_paths\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
