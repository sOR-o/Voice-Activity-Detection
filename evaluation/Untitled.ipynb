{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba4adb37-92bd-4fca-b742-f7443d783a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice: ffmpeg is not installed. torchaudio is used to load audio\n",
      "If you want to use ffmpeg backend to load audio, please install it by:\n",
      "\tsudo apt install ffmpeg # ubuntu\n",
      "\t# brew install ffmpeg # mac\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from funasr import AutoModel\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from speechbrain.inference.VAD import VAD\n",
    "import seaborn as sns\n",
    "from pyannote.core import Segment\n",
    "from pyannote.audio import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea10b335-180a-4584-81d8-f789c0b447dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to ../models/.cache/master.zip\n",
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.1.3 to v2.3.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/059e96f964841d40f1a5e755bb7223f76666bba4/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.7.1, yours is 2.3.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 14:44:01,953 - modelscope - INFO - PyTorch version 2.3.1 Found.\n",
      "2024-07-04 14:44:01,954 - modelscope - INFO - Loading ast index from /Users/saurabh/.cache/modelscope/ast_indexer\n",
      "2024-07-04 14:44:01,998 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 270895fc7d76b5c7655183a5b4e2f1dd and a total number of 980 components indexed\n",
      "2024-07-04 14:44:03,517 - modelscope - INFO - Use user-specified model revision: v2.0.4\n"
     ]
    }
   ],
   "source": [
    "# silero\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "torch.hub.set_dir('../models/.cache')\n",
    "model_silero, utils_silero = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              onnx=False)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils_silero\n",
    "\n",
    " # pyannote\n",
    "pipeline = Pipeline.from_pretrained (\n",
    "        \"pyannote/voice-activity-detection\",\n",
    "         use_auth_token=\"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\", # Once while downloading the model\n",
    "        cache_dir=\"../models/.cache\"\n",
    "        )\n",
    "\n",
    "# speechbrain\n",
    "vad = VAD.from_hparams(\n",
    "        source=\"speechbrain/vad-crdnn-libriparty\",\n",
    "        savedir=\"../models/.cache\"  # Save the model in a cache folder\n",
    ")\n",
    "\n",
    "# funasr\n",
    "model_funasr = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd34b1e0-2804-4ed7-84fa-595bfceb3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/saurabh/Documents/projects/Voice-Activity-Detection\")\n",
    "\n",
    "from helper import vad_inference_pyannote, print_timestamps_pyannote, run_vad_on_noisy_audio_pyannote, visualize_metrics_vs_SNR_pyannote\n",
    "from helper import vad_inference_funasr, convert_to_timestamps_funasr, run_vad_on_noisy_audio_funasr, visualize_metrics_vs_SNR_funasr\n",
    "from helper import vad_inference_silero, print_timestamps_silero, run_vad_on_noisy_audio_silero, visualize_metrics_vs_SNR_silero\n",
    "from helper import vad_inference_speechbrain, print_timestamps_speechbrain, run_vad_on_noisy_audio_speechbrain, visualize_metrics_vs_SNR_speechbrain\n",
    "from helper import parse_annotations_file, evaluate_vad, add_noise, save_audio, plot_SNR, extract_metrics, visualize_all_metrics, evaluate_vad_cmatrix, plot_confusion_matrices, normalize, compute_composite_score, aggregate_metrics, evaluate_all_models, calculate_best_worst_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "876531a4-4c57-4fdd-92c7-32df91bd4ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = ['../audio-wav/english-01.wav', '../audio-wav/marathi-01.wav', '../audio-wav/kannada-01.wav', '../audio-wav/mix-01.wav']\n",
    "noise_path = '../audio-wav/rain-noise.wav'\n",
    "label_paths = ['../audio-label/english-01.txt', '../audio-label/marathi-01.txt', '../audio-label/kannada-01.txt', '../audio-label/mix-01.txt']\n",
    "\n",
    "annotated_segments = [parse_annotations_file(label_path) for label_path in label_paths] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f100733-701f-40de-b46d-f30f0116dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmatrix_pyannote = []\n",
    "cmatrix_silero = []\n",
    "cmatrix_speechbrain = []\n",
    "cmatrix_funasr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d800ea07-347c-40b1-a562-48141df73928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 27.861: 100%|\u001b[34m██████████████████████████████████████████████████\u001b[0m| 1/1 [00:00<00:00,  1.79it/s]\u001b[0m\n",
      "rtf_avg: 0.008: 100%|\u001b[34m███████████████████████████████████████████████████\u001b[0m| 1/1 [00:00<00:00,  3.29it/s]\u001b[0m\n",
      "rtf_avg: 0.009: 100%|\u001b[34m███████████████████████████████████████████████████\u001b[0m| 1/1 [00:00<00:00,  2.26it/s]\u001b[0m\n",
      "rtf_avg: 0.008: 100%|\u001b[34m███████████████████████████████████████████████████\u001b[0m| 1/1 [00:00<00:00,  3.06it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(audio_paths)):\n",
    "    pyannote = vad_inference_pyannote(pipeline, audio_paths[i])\n",
    "    funasr = vad_inference_funasr(audio_paths[i], model_funasr)\n",
    "    silero = vad_inference_silero(audio_paths[i], model_silero, utils_silero, sampling_rate=SAMPLING_RATE)\n",
    "    speechbrain = vad_inference_speechbrain(audio_paths[i], vad)\n",
    "\n",
    "    pyannote = print_timestamps_pyannote(pyannote)\n",
    "    funasr = convert_to_timestamps_funasr(funasr)\n",
    "    silero = print_timestamps_silero(silero)\n",
    "    speechbrain = print_timestamps_speechbrain(speechbrain)\n",
    "\n",
    "    cmatrix_pyannote.append(evaluate_vad(pyannote, annotated_segments[i]))\n",
    "    cmatrix_silero.append(evaluate_vad(silero, annotated_segments[i]))\n",
    "    cmatrix_speechbrain.append(evaluate_vad(speechbrain, annotated_segments[i]))\n",
    "    cmatrix_funasr.append(evaluate_vad(funasr, annotated_segments[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a06cbfb-f6a4-441c-b860-8d4a7a1f5115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best possible score: 0.8\n",
      "Worst possible score: -0.2\n",
      "{'pyannote': np.float64(0.7438059938833472), 'funasr': np.float64(0.6632817509335788), 'silero': np.float64(0.7208168971793987), 'speechbrain': np.float64(0.7147776580898206)}\n"
     ]
    }
   ],
   "source": [
    "# Define your weights\n",
    "weights = {\n",
    "    'precision': 0.2,\n",
    "    'recall': 0.2,\n",
    "    'f1_score': 0.2,\n",
    "    'accuracy': 0.1,\n",
    "    'specificity': 0.1,\n",
    "    'fdr': -0.1,  # Inverted weight for a metric where lower is better\n",
    "    'miss_rate': -0.1  # Inverted weight for a metric where lower is better\n",
    "}\n",
    "\n",
    "# Calculate best and worst scores\n",
    "best_score, worst_score = calculate_best_worst_scores(weights)\n",
    "print(f\"Best possible score: {best_score}\")\n",
    "print(f\"Worst possible score: {worst_score}\")\n",
    "\n",
    "# Assume the cmatrix lists are available\n",
    "scores = evaluate_all_models(cmatrix_pyannote, cmatrix_funasr, cmatrix_silero, cmatrix_speechbrain)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988509c-c74c-44df-a33a-1e49cbac84cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
