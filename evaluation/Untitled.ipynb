{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b8e0a8e-26e8-4f29-adf6-9bc2061ea28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torchaudio\n",
    "# !pip install -qq pyannote.audio\n",
    "# !pip install speechbrain -q\n",
    "# !pip install -U funasr -q\n",
    "# !pip install -U modelscope -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ecc812-6590-402d-8474-5ea37fe215fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice: ffmpeg is not installed. torchaudio is used to load audio\n",
      "If you want to use ffmpeg backend to load audio, please install it by:\n",
      "\tsudo apt install ffmpeg # ubuntu\n",
      "\t# brew install ffmpeg # mac\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from funasr import AutoModel\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "from speechbrain.inference.VAD import VAD\n",
    "from pyannote.core import Segment\n",
    "from pyannote.audio import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af7fc847-2e8e-4b8d-bfa2-0150a6c1b893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to ../models/.cache/master.zip\n",
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.1.3 to v2.3.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/059e96f964841d40f1a5e755bb7223f76666bba4/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.7.1, yours is 2.3.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 22:39:13,867 - modelscope - INFO - PyTorch version 2.3.1 Found.\n",
      "2024-06-26 22:39:13,868 - modelscope - INFO - Loading ast index from /Users/saurabh/.cache/modelscope/ast_indexer\n",
      "2024-06-26 22:39:13,913 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 270895fc7d76b5c7655183a5b4e2f1dd and a total number of 980 components indexed\n",
      "2024-06-26 22:39:14,929 - modelscope - INFO - Use user-specified model revision: v2.0.4\n"
     ]
    }
   ],
   "source": [
    "# silero\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "torch.hub.set_dir('../models/.cache')\n",
    "model_silero, utils_silero = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              onnx=False)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils_silero\n",
    "\n",
    " # pyannote\n",
    "pipeline = Pipeline.from_pretrained (\n",
    "        \"pyannote/voice-activity-detection\",\n",
    "         use_auth_token=\"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\", # Once while downloading the model\n",
    "        cache_dir=\"../models/.cache\"\n",
    "        )\n",
    "\n",
    "# speechbrain\n",
    "vad = VAD.from_hparams(\n",
    "        source=\"speechbrain/vad-crdnn-libriparty\",\n",
    "        savedir=\"../models/.cache\"  # Save the model in a cache folder\n",
    ")\n",
    "\n",
    "# funasr\n",
    "model_funasr = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36446351-a2c5-4e0f-8b6d-5ad1a6e26f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/saurabh/Documents/projects/Voice-Activity-Detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bfe8dfe-abf6-4b51-be1d-a889944b4cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import vad_inference_pyannote, print_timestamps_pyannote, run_vad_on_noisy_audio_pyannote, visualize_metrics_vs_SNR_pyannote\n",
    "from helper import vad_inference_funasr, convert_to_timestamps_funasr, run_vad_on_noisy_audio_funasr, visualize_metrics_vs_SNR_funasr\n",
    "from helper import vad_inference_silero, print_timestamps_silero, run_vad_on_noisy_audio_silero, visualize_metrics_vs_SNR_silero\n",
    "from helper import vad_inference_speechbrain, print_timestamps_speechbrain, run_vad_on_noisy_audio_speechbrain, visualize_metrics_vs_SNR_speechbrain\n",
    "from helper import parse_annotations_file, evaluate_vad, add_noise, save_audio, plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "688a5c3a-f5ba-485c-a40c-a447a227b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = '../audio-wav/english-01.wav'\n",
    "noise_path = '../audio-wav/rain-noise.wav'\n",
    "label_path = '../audio-label/english-01.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "771b2822-f2a0-4a33-a3c8-df07b33ed6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_segments = parse_annotations_file(label_path) # Formate the label properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d4f09c2-c5e4-4859-9f1d-e9ccf0388a89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m res_pyannote \u001b[38;5;241m=\u001b[39m visualize_metrics_vs_SNR_pyannote(pipeline, audio_path, noise_path, annotated_segments, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m25\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres_pyannote\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/Voice-Activity-Detection/helper/vad.py:128\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(metrics_results)\u001b[0m\n\u001b[1;32m    125\u001b[0m fdr \u001b[38;5;241m=\u001b[39m [result[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfdr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m metrics_results]\n\u001b[1;32m    126\u001b[0m miss_rate \u001b[38;5;241m=\u001b[39m [result[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmiss_rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m metrics_results]\n\u001b[0;32m--> 128\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m15\u001b[39m))\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Precision vs SNR\u001b[39;00m\n\u001b[1;32m    131\u001b[0m axs[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(snrs, precision, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "res_pyannote = visualize_metrics_vs_SNR_pyannote(pipeline, audio_path, noise_path, annotated_segments, 5, 25)\n",
    "plot(res_pyannote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a12a6d41-fcca-4421-9ada-58fb58d4b4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function get_speech_timestamps at 0x169dd5800>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897456c1-df34-4909-8f02-f23f01df8b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
