{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb85cd9-44b7-462c-9aab-bb5b0e192f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U funasr -q\n",
    "# !pip install -U modelscope -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112e9915-e66f-4a3a-b49e-ac1d6d5a54f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice: ffmpeg is not installed. torchaudio is used to load audio\n",
      "If you want to use ffmpeg backend to load audio, please install it by:\n",
      "\tsudo apt install ffmpeg # ubuntu\n",
      "\t# brew install ffmpeg # mac\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from funasr import AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "396c5ae1-2a33-47c2-9808-fcc0fc012d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the cache directory\n",
    "torch.hub.set_dir('.cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a38420ef-433e-45bd-893b-ebc579843738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 16:16:10,373 - modelscope - INFO - Use user-specified model revision: v2.0.4\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf4bf0-ef0a-4b90-8094-0d9370d3678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vad_inference(audio_path):\n",
    "    res = model.generate(input=audio_path) # Generate speech activity detection\n",
    "    return res[0]['value']  # Assuming timestamps are stored in the first element\n",
    "\n",
    "def convert_to_timestamps(timestamps):\n",
    "    output_segments = []\n",
    "    start = 0\n",
    "    for timestamp in timestamps:\n",
    "        new_start = timestamp[0]/10**3\n",
    "        new_end = timestamp[1]/10**3\n",
    "        dict = {'speech': [new_start, new_end]}\n",
    "        output_segments.append(dict)\n",
    "\n",
    "    return output_segments\n",
    "\n",
    "def input_type_for_visualize_timestamps(timestamps):\n",
    "    # Convert results to list of dictionaries with start and end times in seconds\n",
    "    speech_timestamps = [{'start': segment[0]/10**3, 'end': segment[1]/10**3} for segment in timestamps]\n",
    "    return speech_timestamps\n",
    "\n",
    "# Function to visualize speech timestamps\n",
    "def visualize_timestamps(speech_timestamps):\n",
    "    fig, ax = plt.subplots(figsize=(12, 2))\n",
    "    for timestamp in speech_timestamps:\n",
    "        ax.plot([timestamp['start'], timestamp['end']], [1, 1], color='black', linewidth=1)\n",
    "    ax.set_xlim([0, max(timestamp['end'] for timestamp in speech_timestamps)])\n",
    "    ax.set_ylim([0.5, 1.5])\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('Speech Activity Detection')\n",
    "    plt.show()\n",
    "\n",
    "# Function to print speech timestamps\n",
    "def print_timestamps(speech_timestamps):\n",
    "    for idx, timestamp in enumerate(speech_timestamps):\n",
    "        start_time = timestamp['start']\n",
    "        end_time = timestamp['end']\n",
    "        duration = end_time - start_time\n",
    "        print(f\"Start: {start_time:.2f}s, End: {end_time:.2f}s, Duration: {duration:.2f}s\")\n",
    "\n",
    "def parse_annotations_file(file_path):\n",
    "    annotated_segments = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        # print(lines)\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()  # Remove leading/trailing whitespace including '\\n'\n",
    "        parts = line.split()\n",
    "        if len(parts) == 3:\n",
    "            start_time = float(parts[0].rstrip('s'))  # Remove 's' from seconds\n",
    "            end_time = float(parts[1].rstrip('s'))  # Remove 's' from seconds\n",
    "            label = parts[2]\n",
    "\n",
    "            if label == 'speech':  # Correcting typo 'speach' to 'speech'\n",
    "                annotated_segments.append({'speech': [start_time, end_time]})\n",
    "            elif label == 'notspeech':\n",
    "                annotated_segments.append({'notspeech': [start_time, end_time]})\n",
    "            else:\n",
    "                # Handle other labels if needed\n",
    "                pass\n",
    "\n",
    "    return annotated_segments\n",
    "\n",
    "def evaluate_vad(output_segments, annotated_segments):\n",
    "    output_intervals = [(seg['speech'][0], seg['speech'][1], 'speech') for seg in output_segments]\n",
    "    annotated_intervals = []\n",
    "    for seg in annotated_segments:\n",
    "        if 'speech' in seg:\n",
    "            annotated_intervals.append((seg['speech'][0], seg['speech'][1], 'speech'))\n",
    "        elif 'notspeech' in seg:\n",
    "            annotated_intervals.append((seg['notspeech'][0], seg['notspeech'][1], 'notspeech'))\n",
    "\n",
    "    resolution = 0.01\n",
    "    max_time = max(max(end for _, end, _ in annotated_intervals), max(end for _, end, _ in output_intervals))\n",
    "    time_points = [i * resolution for i in range(int(max_time / resolution) + 1)]\n",
    "\n",
    "    y_true = ['notspeech'] * len(time_points)\n",
    "    y_pred = ['notspeech'] * len(time_points)\n",
    "\n",
    "    for start, end, label in annotated_intervals:\n",
    "        for i in range(int(start / resolution), int(end / resolution)):\n",
    "            y_true[i] = label\n",
    "\n",
    "    for start, end, label in output_intervals:\n",
    "        for i in range(int(start / resolution), int(end / resolution)):\n",
    "            y_pred[i] = label\n",
    "\n",
    "    y_true_binary = [1 if label == 'speech' else 0 for label in y_true]\n",
    "    y_pred_binary = [1 if label == 'speech' else 0 for label in y_pred]\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    precision = precision_score(y_true_binary, y_pred_binary)\n",
    "    recall = recall_score(y_true_binary, y_pred_binary)\n",
    "    f1 = f1_score(y_true_binary, y_pred_binary)\n",
    "    accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "\n",
    "    # Calculate Specificity\n",
    "    true_negatives = sum(1 for true, pred in zip(y_true_binary, y_pred_binary) if true == 0 and pred == 0)\n",
    "    false_positives = sum(1 for true, pred in zip(y_true_binary, y_pred_binary) if true == 0 and pred == 1)\n",
    "    specificity = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) > 0 else 0\n",
    "\n",
    "    # Calculate False Discovery Rate (FDR)\n",
    "    false_positives = sum(1 for true, pred in zip(y_true_binary, y_pred_binary) if true == 0 and pred == 1)\n",
    "    true_positives = sum(1 for true, pred in zip(y_true_binary, y_pred_binary) if true == 1 and pred == 1)\n",
    "    fdr = false_positives / (false_positives + true_positives) if (false_positives + true_positives) > 0 else 0\n",
    "\n",
    "    # Calculate Miss Rate (False Negative Rate)\n",
    "    false_negatives = sum(1 for true, pred in zip(y_true_binary, y_pred_binary) if true == 1 and pred == 0)\n",
    "    true_positives = sum(1 for true, pred in zip(y_true_binary, y_pred_binary) if true == 1 and pred == 1)\n",
    "    miss_rate = false_negatives / (false_negatives + true_positives) if (false_negatives + true_positives) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'specificity': specificity,\n",
    "        'fdr': fdr,\n",
    "        'miss_rate': miss_rate\n",
    "    }\n",
    "\n",
    "def visualize_comparison(output_segments, annotated_segments):\n",
    "    fig, ax = plt.subplots(figsize=(12, 2))\n",
    "\n",
    "    # Visualize annotated segments\n",
    "    for seg in annotated_segments:\n",
    "        if 'speech' in seg:\n",
    "            ax.plot([seg['speech'][0], seg['speech'][1]], [2, 2], color='blue', linewidth=2, label='Annotated Speech' if 'Annotated Speech' not in ax.get_legend_handles_labels()[1] else \"\")\n",
    "        elif 'notspeech' in seg:\n",
    "            ax.plot([seg['notspeech'][0], seg['notspeech'][1]], [2, 2], color='red', linewidth=2, label='Annotated Non-Speech' if 'Annotated Non-Speech' not in ax.get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "    # Visualize output segments\n",
    "    for seg in output_segments:\n",
    "        ax.plot([seg['speech'][0], seg['speech'][1]], [1, 1], color='green', linewidth=2, label='Predicted Speech' if 'Predicted Speech' not in ax.get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "    ax.set_xlim([0, max(max(seg['speech'][1] for seg in output_segments), max(max(s.values())[1] for s in annotated_segments))])\n",
    "    ax.set_ylim([0.5, 2.5])\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_yticks([1, 2])\n",
    "    ax.set_yticklabels(['Predicted', 'Annotated'])\n",
    "    ax.set_title('Speech Activity Detection: Annotated vs Predicted')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def visualize_comparison_stacked(output_segments, annotated_segments):\n",
    "    fig, ax = plt.subplots(figsize=(12, 2))\n",
    "\n",
    "    # Define colors\n",
    "    colors = {\n",
    "        'annotated_speech': 'blue',\n",
    "        'annotated_nonspeech': 'red',\n",
    "        'predicted_speech': 'green'\n",
    "    }\n",
    "\n",
    "    # Add annotated segments to the plot\n",
    "    for seg in annotated_segments:\n",
    "        if 'speech' in seg:\n",
    "            ax.broken_barh([(seg['speech'][0], seg['speech'][1] - seg['speech'][0])], (2, 1), facecolors=colors['annotated_speech'], edgecolor='none')\n",
    "        elif 'notspeech' in seg:\n",
    "            ax.broken_barh([(seg['notspeech'][0], seg['notspeech'][1] - seg['notspeech'][0])], (2, 1), facecolors=colors['annotated_nonspeech'], edgecolor='none')\n",
    "\n",
    "    # Add predicted segments to the plot\n",
    "    for seg in output_segments:\n",
    "        ax.broken_barh([(seg['speech'][0], seg['speech'][1] - seg['speech'][0])], (1, 1), facecolors=colors['predicted_speech'], edgecolor='none')\n",
    "\n",
    "    # Add labels, legend, and limits\n",
    "    ax.set_xlim(0, max(max(seg['speech'][1] for seg in output_segments), max(max(s.values())[1] for s in annotated_segments)))\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_yticks([1.5, 2.5])\n",
    "    ax.set_yticklabels(['Predicted', 'Annotated'])\n",
    "    ax.set_title('Speech Activity Detection: Annotated vs Predicted')\n",
    "\n",
    "    # Create a custom legend\n",
    "    annotated_speech_patch = mpatches.Patch(color=colors['annotated_speech'], label='Annotated Speech')\n",
    "    annotated_nonspeech_patch = mpatches.Patch(color=colors['annotated_nonspeech'], label='Annotated Non-Speech')\n",
    "    predicted_speech_patch = mpatches.Patch(color=colors['predicted_speech'], label='Predicted Speech')\n",
    "    ax.legend(handles=[annotated_speech_patch, annotated_nonspeech_patch, predicted_speech_patch], loc='upper right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def add_noise(audio_path, noise_path, snr):\n",
    "    # Load audio and noise\n",
    "    audio, sr = sf.read(audio_path)\n",
    "    noise, _ = sf.read(noise_path)\n",
    "\n",
    "    # Truncate or pad noise to match audio length\n",
    "    if len(noise) > len(audio):\n",
    "        noise = noise[:len(audio)]\n",
    "    else:\n",
    "        noise = np.pad(noise, (0, len(audio) - len(noise)), 'wrap')\n",
    "\n",
    "    # Calculate signal power and noise power\n",
    "    audio_power = np.sum(audio ** 2) / len(audio)\n",
    "    noise_power = np.sum(noise ** 2) / len(noise)\n",
    "\n",
    "    # Calculate required noise level to achieve the desired SNR\n",
    "    required_noise_power = audio_power / (10 ** (snr / 10))\n",
    "    noise_scaling_factor = np.sqrt(required_noise_power / noise_power)\n",
    "    noisy_audio = audio + noise_scaling_factor * noise\n",
    "\n",
    "    return noisy_audio, sr\n",
    "\n",
    "def run_vad_on_noisy_audio(audio_path, noise_path, snr):\n",
    "    noisy_audio, sr = add_noise(audio_path, noise_path, snr)\n",
    "    noisy_audio_path = \"noisy_audio.wav\"\n",
    "    save_audio(noisy_audio, sr, noisy_audio_path)\n",
    "    output = vad_inference(noisy_audio_path)\n",
    "    return output\n",
    "\n",
    "def save_audio(audio, sr, path):\n",
    "    sf.write(path, audio, sr)\n",
    "\n",
    "def visualize_metrics_vs_SNR(low, high):\n",
    "    snr_levels = [dp for dp in range(low, high)]\n",
    "    metrics_results = []\n",
    "\n",
    "    for snr in snr_levels:\n",
    "        output = run_vad_on_noisy_audio(audio_path, noise_path, snr)\n",
    "        output_segments = convert_to_timestamps(output)\n",
    "        metrics = evaluate_vad(output_segments, annotated_segments)\n",
    "        metrics_results.append((snr, metrics))\n",
    "\n",
    "    snrs = [result[0] for result in metrics_results]\n",
    "    precision = [result[1]['precision'] for result in metrics_results]\n",
    "    recall = [result[1]['recall'] for result in metrics_results]\n",
    "    f1_score = [result[1]['f1_score'] for result in metrics_results]\n",
    "    accuracy = [result[1]['accuracy'] for result in metrics_results]\n",
    "    specificity = [result[1]['specificity'] for result in metrics_results]\n",
    "    fdr = [result[1]['fdr'] for result in metrics_results]\n",
    "    miss_rate = [result[1]['miss_rate'] for result in metrics_results]\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "    # Precision vs SNR\n",
    "    axs[0, 0].plot(snrs, precision, marker='o', color='b', label='Precision')\n",
    "    axs[0, 0].set_xlabel('SNR (dB)')\n",
    "    axs[0, 0].set_ylabel('Precision')\n",
    "    axs[0, 0].set_title('Precision vs SNR')\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].grid(True)\n",
    "\n",
    "    # Recall vs SNR\n",
    "    axs[0, 1].plot(snrs, recall, marker='o', color='g', label='Recall')\n",
    "    axs[0, 1].set_xlabel('SNR (dB)')\n",
    "    axs[0, 1].set_ylabel('Recall')\n",
    "    axs[0, 1].set_title('Recall vs SNR')\n",
    "    axs[0, 1].legend()\n",
    "    axs[0, 1].grid(True)\n",
    "\n",
    "    # F1-score vs SNR\n",
    "    axs[0, 2].plot(snrs, f1_score, marker='o', color='r', label='F1-score')\n",
    "    axs[0, 2].set_xlabel('SNR (dB)')\n",
    "    axs[0, 2].set_ylabel('F1-score')\n",
    "    axs[0, 2].set_title('F1-score vs SNR')\n",
    "    axs[0, 2].legend()\n",
    "    axs[0, 2].grid(True)\n",
    "\n",
    "    # Accuracy vs SNR\n",
    "    axs[1, 0].plot(snrs, accuracy, marker='o', color='m', label='Accuracy')\n",
    "    axs[1, 0].set_xlabel('SNR (dB)')\n",
    "    axs[1, 0].set_ylabel('Accuracy')\n",
    "    axs[1, 0].set_title('Accuracy vs SNR')\n",
    "    axs[1, 0].legend()\n",
    "    axs[1, 0].grid(True)\n",
    "\n",
    "    # Specificity vs SNR\n",
    "    axs[1, 1].plot(snrs, specificity, marker='o', color='c', label='Specificity')\n",
    "    axs[1, 1].set_xlabel('SNR (dB)')\n",
    "    axs[1, 1].set_ylabel('Specificity')\n",
    "    axs[1, 1].set_title('Specificity vs SNR')\n",
    "    axs[1, 1].legend()\n",
    "    axs[1, 1].grid(True)\n",
    "\n",
    "    # FDR vs SNR\n",
    "    axs[1, 2].plot(snrs, fdr, marker='o', color='y', label='FDR')\n",
    "    axs[1, 2].set_xlabel('SNR (dB)')\n",
    "    axs[1, 2].set_ylabel('FDR')\n",
    "    axs[1, 2].set_title('False Discovery Rate (FDR) vs SNR')\n",
    "    axs[1, 2].legend()\n",
    "    axs[1, 2].grid(True)\n",
    "\n",
    "    # Miss Rate vs SNR\n",
    "    axs[2, 0].plot(snrs, miss_rate, marker='o', color='k', label='Miss Rate')\n",
    "    axs[2, 0].set_xlabel('SNR (dB)')\n",
    "    axs[2, 0].set_ylabel('Miss Rate')\n",
    "    axs[2, 0].set_title('Miss Rate (False Negative Rate) vs SNR')\n",
    "    axs[2, 0].legend()\n",
    "    axs[2, 0].grid(True)\n",
    "\n",
    "    # Hide the last subplot\n",
    "    axs[2, 1].axis('off')\n",
    "    axs[2, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
